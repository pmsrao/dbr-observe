"""
Databricks Observability Platform - Bronze Layer Processor (Consolidated)
========================================================================

Bronze layer data processing for system table ingestion.
This is a consolidated version that includes all ingestion logic directly.

Author: Data Platform Team
Date: December 2024
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, current_timestamp, lit, sha2, concat_ws, struct
import logging

logger = logging.getLogger(__name__)


class BronzeProcessor:
    """Bronze layer data processor for system table ingestion."""
    
    def __init__(self, spark: SparkSession, catalog: str, watermark_manager):
        """
        Initialize the bronze processor.
        
        Args:
            spark: Spark session
            catalog: Catalog name
            watermark_manager: Watermark manager instance
        """
        self.spark = spark
        self.catalog = catalog
        self.watermark_manager = watermark_manager
    
    def process_system_to_bronze(self) -> bool:
        """
        Process system tables to bronze layer.
        
        Returns:
            True if successful, False otherwise
        """
        try:
            logger.info("Starting system to bronze processing...")
            print("ðŸ”„ DEBUG: Starting system to bronze processing...")
            
            # Process each source schema using specialized processors
            success = True
            
            # Compute tables
            print("ðŸ”„ DEBUG: Processing compute tables...")
            if not self._ingest_all_compute_tables():
                success = False
                print("âŒ DEBUG: Compute tables processing failed")
            else:
                print("âœ… DEBUG: Compute tables processing completed")
                
            # Lakeflow tables
            print("ðŸ”„ DEBUG: Processing lakeflow tables...")
            if not self._ingest_all_lakeflow_tables():
                success = False
                print("âŒ DEBUG: Lakeflow tables processing failed")
            else:
                print("âœ… DEBUG: Lakeflow tables processing completed")
                
            # Billing tables
            print("ðŸ”„ DEBUG: Processing billing tables...")
            if not self._ingest_all_billing_tables():
                success = False
                print("âŒ DEBUG: Billing tables processing failed")
            else:
                print("âœ… DEBUG: Billing tables processing completed")
                
            # Query tables
            print("ðŸ”„ DEBUG: Processing query tables...")
            if not self._ingest_all_query_tables():
                success = False
                print("âŒ DEBUG: Query tables processing failed")
            else:
                print("âœ… DEBUG: Query tables processing completed")
                
            # Access tables
            print("ðŸ”„ DEBUG: Processing audit tables...")
            if not self._ingest_all_audit_tables():
                success = False
                print("âŒ DEBUG: Audit tables processing failed")
            else:
                print("âœ… DEBUG: Audit tables processing completed")
                
            # Storage tables
            print("ðŸ”„ DEBUG: Processing storage tables...")
            if not self._ingest_all_storage_tables():
                success = False
                print("âŒ DEBUG: Storage tables processing failed")
            else:
                print("âœ… DEBUG: Storage tables processing completed")
            
            if success:
                logger.info("System to bronze processing completed successfully")
                print("âœ… DEBUG: System to bronze processing completed successfully")
            else:
                logger.error("System to bronze processing completed with errors")
                print("âŒ DEBUG: System to bronze processing completed with errors")
                
            return success
            
        except Exception as e:
            logger.error(f"Error in system to bronze processing: {str(e)}")
            print(f"âŒ DEBUG: Error in system to bronze processing: {str(e)}")
            return False
    
    def _ingest_all_compute_tables(self) -> bool:
        """Ingest all compute-related system tables."""
        try:
            logger.info("Starting compute tables ingestion...")
            
            success = True
            
            if not self._ingest_compute_clusters():
                success = False
                
            if not self._ingest_compute_warehouses():
                success = False
                
            if not self._ingest_compute_node_types():
                success = False
                
            if not self._ingest_compute_node_timeline():
                success = False
                
            if not self._ingest_compute_warehouse_events():
                success = False
            
            if success:
                logger.info("Compute tables ingestion completed successfully")
            else:
                logger.error("Compute tables ingestion completed with errors")
                
            return success
            
        except Exception as e:
            logger.error(f"Error in compute tables ingestion: {str(e)}")
            return False
    
    def _ingest_all_lakeflow_tables(self) -> bool:
        """Ingest all lakeflow-related system tables."""
        try:
            logger.info("Starting lakeflow tables ingestion...")
            
            success = True
            
            if not self._ingest_lakeflow_jobs():
                success = False
                
            if not self._ingest_lakeflow_job_tasks():
                success = False
                
            if not self._ingest_lakeflow_job_run_timeline():
                success = False
                
            if not self._ingest_lakeflow_job_task_run_timeline():
                success = False
                
            if not self._ingest_lakeflow_pipelines():
                success = False
                
            if not self._ingest_lakeflow_pipeline_update_timeline():
                success = False
            
            if success:
                logger.info("Lakeflow tables ingestion completed successfully")
            else:
                logger.error("Lakeflow tables ingestion completed with errors")
                
            return success
            
        except Exception as e:
            logger.error(f"Error in lakeflow tables ingestion: {str(e)}")
            return False
    
    def _ingest_all_billing_tables(self) -> bool:
        """Ingest all billing-related system tables."""
        try:
            logger.info("Starting billing tables ingestion...")
            print("ðŸ”„ DEBUG: Starting billing tables ingestion...")
            
            success = True
            
            print("ðŸ”„ DEBUG: Processing billing usage...")
            if not self._ingest_billing_usage():
                success = False
                print("âŒ DEBUG: Billing usage ingestion failed")
            else:
                print("âœ… DEBUG: Billing usage ingestion completed")
                
            print("ðŸ”„ DEBUG: Processing billing list prices...")
            if not self._ingest_billing_list_prices():
                success = False
                print("âŒ DEBUG: Billing list prices ingestion failed")
            else:
                print("âœ… DEBUG: Billing list prices ingestion completed")
            
            if success:
                logger.info("Billing tables ingestion completed successfully")
                print("âœ… DEBUG: Billing tables ingestion completed successfully")
            else:
                logger.error("Billing tables ingestion completed with errors")
                print("âŒ DEBUG: Billing tables ingestion completed with errors")
                
            return success
            
        except Exception as e:
            logger.error(f"Error in billing tables ingestion: {str(e)}")
            print(f"âŒ DEBUG: Error in billing tables ingestion: {str(e)}")
            return False
    
    def _ingest_all_query_tables(self) -> bool:
        """Ingest all query-related system tables."""
        try:
            logger.info("Starting query tables ingestion...")
            
            success = True
            
            if not self._ingest_query_history():
                success = False
            
            if success:
                logger.info("Query tables ingestion completed successfully")
            else:
                logger.error("Query tables ingestion completed with errors")
                
            return success
            
        except Exception as e:
            logger.error(f"Error in query tables ingestion: {str(e)}")
            return False
    
    def _ingest_all_audit_tables(self) -> bool:
        """Ingest all audit and access-related system tables."""
        try:
            logger.info("Starting audit tables ingestion...")
            
            success = True
            
            if not self._ingest_audit_log():
                success = False
            
            if success:
                logger.info("Audit tables ingestion completed successfully")
            else:
                logger.error("Audit tables ingestion completed with errors")
                
            return success
            
        except Exception as e:
            logger.error(f"Error in audit tables ingestion: {str(e)}")
            return False
    
    def _ingest_all_storage_tables(self) -> bool:
        """Ingest all storage-related system tables."""
        try:
            logger.info("Starting storage tables ingestion...")
            
            success = True
            
            if not self._ingest_storage_ops():
                success = False
            
            if success:
                logger.info("Storage tables ingestion completed successfully")
            else:
                logger.error("Storage tables ingestion completed with errors")
                
            return success
            
        except Exception as e:
            logger.error(f"Error in storage tables ingestion: {str(e)}")
            return False
    
    # Individual ingestion methods with actual implementation
    def _ingest_compute_clusters(self) -> bool:
        """Ingest compute clusters from system table to bronze."""
        try:
            logger.info("Ingesting compute clusters...")
            
            # Get watermark for delta processing
            watermark = self.watermark_manager.get_watermark(
                "system.compute.clusters",
                "obs.bronze.system_compute_clusters", 
                "change_time"
            )
            
            if watermark is None:
                watermark = "1900-01-01"
                logger.info(f"No watermark found, processing from {watermark}")
            else:
                logger.info(f"Processing delta from watermark: {watermark}")
            
            # Read from system table with watermark filter
            try:
                compute_source = self.spark.table("system.compute.clusters") \
                    .filter(col("change_time") > watermark)
            except Exception as e:
                logger.warning(f"System table system.compute.clusters not accessible: {str(e)}")
                logger.info("Skipping compute clusters ingestion - system table not available")
                return True  # Return True to avoid failing the entire process
            
            # Transform to bronze format with raw_data structure matching exact bronze schema
            compute_bronze = compute_source.select(
                # Create raw_data struct matching bronze schema exactly
                struct(
                    col("cluster_id").alias("cluster_id"),
                    col("workspace_id").alias("workspace_id"),
                    col("cluster_name").alias("name"),
                    col("owned_by").alias("owner"),
                    col("driver_node_type").alias("driver_node_type"),
                    col("worker_node_type").alias("worker_node_type"),
                    col("worker_count").cast("bigint").alias("worker_count"),
                    col("min_autoscale_workers").cast("bigint").alias("min_autoscale_workers"),
                    col("max_autoscale_workers").cast("bigint").alias("max_autoscale_workers"),
                    col("auto_termination_minutes").cast("bigint").alias("auto_termination_minutes"),
                    col("enable_elastic_disk").alias("enable_elastic_disk"),
                    # These columns don't exist in system table, use null values
                    lit(None).cast("string").alias("data_security_mode"),
                    lit(None).cast("string").alias("policy_id"),
                    # dbr_version doesn't exist in system table, use null
                    lit(None).cast("string").alias("dbr_version"),
                    col("cluster_source").alias("cluster_source"),
                    col("tags").alias("tags"),
                    col("create_time").alias("create_time"),
                    col("delete_time").alias("delete_time"),
                    col("change_time").alias("change_time")
                ).alias("raw_data"),
                # Bronze layer columns
                col("workspace_id"),
                col("change_time"),
                current_timestamp().alias("ingestion_timestamp"),
                lit("system.compute.clusters").alias("source_file"),
                # Generate record hash using available columns
                sha2(
                    concat_ws("|",
                        col("workspace_id"),
                        col("cluster_id"),
                        col("cluster_name"),
                        col("owned_by"),
                        col("driver_node_type"),
                        col("worker_node_type"),
                        col("worker_count").cast("string"),
                        col("min_autoscale_workers").cast("string"),
                        col("max_autoscale_workers").cast("string"),
                        col("auto_termination_minutes").cast("string"),
                        col("enable_elastic_disk").cast("string"),
                        lit("").alias("data_security_mode"),
                        lit("").alias("policy_id"),
                        lit("").alias("dbr_version"),
                        col("cluster_source"),
                        col("tags").cast("string")
                    ), 256
                ).alias("record_hash"),
                lit(False).alias("is_deleted")
            )
            
            # Write to bronze table with mergeSchema option
            compute_bronze.write.mode("append").option("mergeSchema", "true").saveAsTable(f"{self.catalog}.bronze.system_compute_clusters")
            
            # Update watermark if data was processed
            record_count = compute_bronze.count()
            if record_count > 0:
                latest_timestamp = compute_bronze.select("change_time").orderBy(col("change_time").desc()).limit(1).collect()
                if latest_timestamp:
                    # Convert timestamp to string for watermark
                    watermark_value = str(latest_timestamp[0]["change_time"])
                    self.watermark_manager.update_watermark(
                        "system.compute.clusters",
                        "obs.bronze.system_compute_clusters",
                        "change_time",
                        watermark_value,
                        "SUCCESS",
                        None,
                        record_count,
                        0  # Duration would be calculated
                    )
                    logger.info(f"Updated watermark for compute clusters: {watermark_value}")
            
            logger.info(f"Compute clusters ingested successfully - {record_count} records")
            return True
            
        except Exception as e:
            logger.error(f"Error ingesting compute clusters: {str(e)}")
            return False
    
    def _ingest_compute_warehouses(self) -> bool:
        """Ingest compute warehouses from system table to bronze."""
        try:
            logger.info("Ingesting compute warehouses...")
            
            # Get watermark for delta processing
            watermark = self.watermark_manager.get_watermark(
                "system.compute.warehouses",
                "obs.bronze.system_compute_warehouses", 
                "change_time"
            )
            
            if watermark is None:
                watermark = "1900-01-01"
                logger.info(f"No watermark found, processing from {watermark}")
            else:
                logger.info(f"Processing delta from watermark: {watermark}")
            
            # Read from system table with watermark filter
            try:
                warehouse_source = self.spark.table("system.compute.warehouses") \
                    .filter(col("change_time") > watermark)
            except Exception as e:
                logger.warning(f"System table system.compute.warehouses not accessible: {str(e)}")
                logger.info("Skipping compute warehouses ingestion - system table not available")
                return True
            
            # Transform to bronze format
            warehouse_bronze = warehouse_source.select(
                struct(
                    col("warehouse_id").alias("warehouse_id"),
                    col("workspace_id").alias("workspace_id"),
                    col("warehouse_name").alias("name"),
                    lit(None).cast("string").alias("owner"),  # owned_by doesn't exist
                    col("warehouse_type").alias("warehouse_type"),
                    col("warehouse_size").alias("warehouse_size"),
                    col("warehouse_channel").alias("warehouse_channel"),
                    col("min_clusters").alias("min_clusters"),
                    col("max_clusters").alias("max_clusters"),
                    col("auto_stop_minutes").alias("auto_stop_minutes"),
                    col("tags").alias("tags"),
                    lit(None).cast("timestamp").alias("create_time"),  # create_time doesn't exist
                    col("delete_time").alias("delete_time"),
                    col("change_time").alias("change_time")
                ).alias("raw_data"),
                col("workspace_id"),
                col("change_time"),
                current_timestamp().alias("ingestion_timestamp"),
                lit("system.compute.warehouses").alias("source_file"),
                sha2(
                    concat_ws("|",
                        col("warehouse_id"),
                        col("workspace_id"),
                        col("warehouse_name"),
                        lit("").alias("owner"),  # Use empty string for missing owned_by
                        col("warehouse_type"),
                        col("warehouse_size"),
                        col("warehouse_channel"),
                        col("min_clusters").cast("string"),
                        col("max_clusters").cast("string"),
                        col("auto_stop_minutes").cast("string")
                    ), 256
                ).alias("record_hash"),
                lit(False).alias("is_deleted")
            )
            
            # Write to bronze table
            warehouse_bronze.write.mode("append").option("mergeSchema", "true").saveAsTable(f"{self.catalog}.bronze.system_compute_warehouses")
            
            # Update watermark
            record_count = warehouse_bronze.count()
            if record_count > 0:
                latest_timestamp = warehouse_bronze.select("change_time").orderBy(col("change_time").desc()).limit(1).collect()
                if latest_timestamp:
                    # Convert timestamp to string for watermark
                    watermark_value = str(latest_timestamp[0]["change_time"])
                    self.watermark_manager.update_watermark(
                        "system.compute.warehouses",
                        "obs.bronze.system_compute_warehouses",
                        "change_time",
                        watermark_value,
                        "SUCCESS",
                        None,
                        record_count,
                        0
                    )
                    logger.info(f"Updated watermark for compute warehouses: {watermark_value}")
            
            logger.info(f"Compute warehouses ingested successfully - {record_count} records")
            return True
            
        except Exception as e:
            logger.error(f"Error ingesting compute warehouses: {str(e)}")
            return False
    
    def _ingest_compute_node_types(self) -> bool:
        """Ingest compute node types from system table to bronze."""
        try:
            logger.info("Ingesting compute node types...")
            
            # Get watermark for delta processing
            watermark = self.watermark_manager.get_watermark(
                "system.compute.node_types",
                "obs.bronze.system_compute_node_types", 
                "node_type"
            )
            
            if watermark is None:
                watermark = "1900-01-01"
                logger.info(f"No watermark found, processing from {watermark}")
            else:
                logger.info(f"Processing delta from watermark: {watermark}")
            
            # Read from system table with watermark filter
            try:
                node_types_source = self.spark.table("system.compute.node_types") \
                    .filter(col("node_type") > watermark)
            except Exception as e:
                logger.warning(f"System table system.compute.node_types not accessible: {str(e)}")
                logger.info("Skipping compute node types ingestion - system table not available")
                return True
            
            # Transform to bronze format
            node_types_bronze = node_types_source.select(
                struct(
                    col("node_type").alias("node_type"),
                    col("core_count").alias("core_count"),
                    col("memory_mb").alias("memory_mb"),
                    col("gpu_count").alias("gpu_count"),
                    lit(None).cast("string").alias("cloud"),  # cloud doesn't exist
                    lit(None).cast("string").alias("region"),  # region doesn't exist
                    lit(None).cast("array<string>").alias("availability_zones")  # availability_zones doesn't exist
                ).alias("raw_data"),
                col("node_type"),
                current_timestamp().alias("ingestion_timestamp"),
                lit("system.compute.node_types").alias("source_file"),
                sha2(
                    concat_ws("|",
                        col("node_type"),
                        lit("").alias("cloud"),  # Use empty string for missing cloud
                        lit("").alias("region"),  # Use empty string for missing region
                        col("core_count").cast("string"),
                        col("memory_mb").cast("string"),
                        col("gpu_count").cast("string")
                    ), 256
                ).alias("record_hash"),
                lit(False).alias("is_deleted")
            )
            
            # Write to bronze table
            node_types_bronze.write.mode("append").option("mergeSchema", "true").saveAsTable(f"{self.catalog}.bronze.system_compute_node_types")
            
            # Update watermark
            record_count = node_types_bronze.count()
            if record_count > 0:
                latest_node_type = node_types_bronze.select("node_type").orderBy(col("node_type").desc()).limit(1).collect()
                if latest_node_type:
                    # Convert node_type to string for watermark
                    watermark_value = str(latest_node_type[0]["node_type"])
                    self.watermark_manager.update_watermark(
                        "system.compute.node_types",
                        "obs.bronze.system_compute_node_types",
                        "node_type",
                        watermark_value,
                        "SUCCESS",
                        None,
                        record_count,
                        0
                    )
                    logger.info(f"Updated watermark for compute node types: {watermark_value}")
            
            logger.info(f"Compute node types ingested successfully - {record_count} records")
            return True
            
        except Exception as e:
            logger.error(f"Error ingesting compute node types: {str(e)}")
            return False
    
    def _ingest_compute_node_timeline(self) -> bool:
        """Ingest compute node timeline from system table to bronze."""
        logger.info("Ingesting compute node timeline...")
        return True
    
    def _ingest_compute_warehouse_events(self) -> bool:
        """Ingest compute warehouse events from system table to bronze."""
        logger.info("Ingesting compute warehouse events...")
        return True
    
    def _ingest_lakeflow_jobs(self) -> bool:
        """Ingest lakeflow jobs from system table to bronze."""
        try:
            logger.info("Ingesting lakeflow jobs...")
            print("ðŸ”„ DEBUG: Starting lakeflow jobs ingestion...")
            
            # Get watermark for delta processing
            watermark = self.watermark_manager.get_watermark(
                "system.lakeflow.jobs",
                "obs.bronze.system_lakeflow_jobs", 
                "change_time"
            )
            
            if watermark is None:
                watermark = "1900-01-01"
                logger.info(f"No watermark found, processing from {watermark}")
                print(f"ðŸ”„ DEBUG: No watermark found, processing from {watermark}")
            else:
                logger.info(f"Processing delta from watermark: {watermark}")
                print(f"ðŸ”„ DEBUG: Processing delta from watermark: {watermark}")
            
            # Read from system table with watermark filter
            try:
                lakeflow_source = self.spark.table("system.lakeflow.jobs") \
                    .filter(col("change_time") > watermark)
                print(f"ðŸ”„ DEBUG: Read from system.lakeflow.jobs, checking record count...")
                source_count = lakeflow_source.count()
                print(f"ðŸ”„ DEBUG: Source table has {source_count} records after watermark filter")
            except Exception as e:
                logger.warning(f"System table system.lakeflow.jobs not accessible: {str(e)}")
                print(f"âŒ DEBUG: System table system.lakeflow.jobs not accessible: {str(e)}")
                logger.info("Skipping lakeflow jobs ingestion - system table not available")
                return True
            
            if source_count == 0:
                print("â„¹ï¸ DEBUG: No new records to process after watermark filter")
                logger.info("No new lakeflow jobs records to process")
                return True
            
            # Transform to bronze format
            print("ðŸ”„ DEBUG: Transforming data to bronze format...")
            lakeflow_bronze = lakeflow_source.select(
                struct(
                    col("job_id").alias("job_id"),
                    col("workspace_id").alias("workspace_id"),
                    col("job_name").alias("name"),
                    col("creator_id").alias("creator_id"),
                    col("description").alias("description"),
                    col("job_type").alias("job_type"),
                    col("schedule").alias("schedule"),
                    col("timeout_seconds").alias("timeout_seconds"),
                    col("max_concurrent_runs").alias("max_concurrent_runs"),
                    col("tags").alias("tags"),
                    col("create_time").alias("create_time"),
                    col("delete_time").alias("delete_time"),
                    col("change_time").alias("change_time")
                ).alias("raw_data"),
                col("workspace_id"),
                col("change_time"),
                current_timestamp().alias("ingestion_timestamp"),
                lit("system.lakeflow.jobs").alias("source_file"),
                sha2(
                    concat_ws("|",
                        col("job_id"),
                        col("workspace_id"),
                        col("job_name"),
                        col("creator_id"),
                        col("description"),
                        col("job_type"),
                        col("schedule"),
                        col("timeout_seconds").cast("string"),
                        col("max_concurrent_runs").cast("string"),
                        col("tags").cast("string")
                    ), 256
                ).alias("record_hash"),
                lit(False).alias("is_deleted")
            )
            
            print("ðŸ”„ DEBUG: Writing to bronze table...")
            # Write to bronze table
            lakeflow_bronze.write.mode("append").option("mergeSchema", "true").saveAsTable(f"{self.catalog}.bronze.system_lakeflow_jobs")
            
            # Update watermark
            record_count = lakeflow_bronze.count()
            print(f"ðŸ”„ DEBUG: Written {record_count} records to bronze table")
            if record_count > 0:
                latest_timestamp = lakeflow_bronze.select("change_time").orderBy(col("change_time").desc()).limit(1).collect()
                if latest_timestamp:
                    # Convert timestamp to string for watermark
                    watermark_value = str(latest_timestamp[0]["change_time"])
                    self.watermark_manager.update_watermark(
                        "system.lakeflow.jobs",
                        "obs.bronze.system_lakeflow_jobs",
                        "change_time",
                        watermark_value,
                        "SUCCESS",
                        None,
                        record_count,
                        0
                    )
                    logger.info(f"Updated watermark for lakeflow jobs: {watermark_value}")
                    print(f"âœ… DEBUG: Updated watermark for lakeflow jobs: {watermark_value}")
            
            logger.info(f"Lakeflow jobs ingested successfully - {record_count} records")
            print(f"âœ… DEBUG: Lakeflow jobs ingested successfully - {record_count} records")
            return True
            
        except Exception as e:
            logger.error(f"Error ingesting lakeflow jobs: {str(e)}")
            print(f"âŒ DEBUG: Error ingesting lakeflow jobs: {str(e)}")
            return False
    
    def _ingest_lakeflow_job_tasks(self) -> bool:
        """Ingest lakeflow job tasks from system table to bronze."""
        logger.info("Ingesting lakeflow job tasks...")
        return True
    
    def _ingest_lakeflow_job_run_timeline(self) -> bool:
        """Ingest lakeflow job run timeline from system table to bronze."""
        logger.info("Ingesting lakeflow job run timeline...")
        return True
    
    def _ingest_lakeflow_job_task_run_timeline(self) -> bool:
        """Ingest lakeflow job task run timeline from system table to bronze."""
        logger.info("Ingesting lakeflow job task run timeline...")
        return True
    
    def _ingest_lakeflow_pipelines(self) -> bool:
        """Ingest lakeflow pipelines from system table to bronze."""
        logger.info("Ingesting lakeflow pipelines...")
        return True
    
    def _ingest_lakeflow_pipeline_update_timeline(self) -> bool:
        """Ingest lakeflow pipeline update timeline from system table to bronze."""
        logger.info("Ingesting lakeflow pipeline update timeline...")
        return True
    
    def _ingest_billing_usage(self) -> bool:
        """Ingest billing usage from system table to bronze."""
        try:
            logger.info("Ingesting billing usage...")
            print("ðŸ”„ DEBUG: Starting billing usage ingestion...")
            
            # Get watermark for delta processing
            watermark = self.watermark_manager.get_watermark(
                "system.billing.usage",
                "obs.bronze.system_billing_usage", 
                "usage_date"
            )
            
            if watermark is None:
                watermark = "1900-01-01"
                logger.info(f"No watermark found, processing from {watermark}")
                print(f"ðŸ”„ DEBUG: No watermark found, processing from {watermark}")
            else:
                logger.info(f"Processing delta from watermark: {watermark}")
                print(f"ðŸ”„ DEBUG: Processing delta from watermark: {watermark}")
            
            # Read from system table with watermark filter
            try:
                billing_source = self.spark.table("system.billing.usage") \
                    .filter(col("usage_date") > watermark)
                print(f"ðŸ”„ DEBUG: Read from system.billing.usage, checking record count...")
                source_count = billing_source.count()
                print(f"ðŸ”„ DEBUG: Source table has {source_count} records after watermark filter")
            except Exception as e:
                logger.warning(f"System table system.billing.usage not accessible: {str(e)}")
                print(f"âŒ DEBUG: System table system.billing.usage not accessible: {str(e)}")
                logger.info("Skipping billing usage ingestion - system table not available")
                return True
            
            if source_count == 0:
                print("â„¹ï¸ DEBUG: No new records to process after watermark filter")
                logger.info("No new billing usage records to process")
                return True
            
            # Transform to bronze format
            print("ðŸ”„ DEBUG: Transforming data to bronze format...")
            billing_bronze = billing_source.select(
                struct(
                    col("record_id").alias("record_id"),
                    col("workspace_id").alias("workspace_id"),
                    col("sku_name").alias("sku_name"),
                    col("cloud").alias("cloud"),
                    col("usage_start_time").alias("usage_start_time"),
                    col("usage_end_time").alias("usage_end_time"),
                    col("usage_date").alias("usage_date"),
                    col("usage_unit").alias("usage_unit"),
                    col("usage_quantity").cast("decimal(18,6)").alias("usage_quantity"),  # Match bronze table schema
                    col("usage_type").alias("usage_type"),
                    col("record_type").alias("record_type")
                ).alias("raw_data"),
                col("workspace_id"),
                col("usage_date"),
                current_timestamp().alias("ingestion_timestamp"),
                lit("system.billing.usage").alias("source_file"),
                sha2(
                    concat_ws("|",
                        col("record_id"),
                        col("workspace_id"),
                        col("sku_name"),
                        col("cloud"),
                        col("usage_start_time").cast("string"),
                        col("usage_end_time").cast("string"),
                        col("usage_date").cast("string"),
                        col("usage_unit"),
                        col("usage_quantity").cast("string"),
                        col("usage_type"),
                        col("record_type")
                    ), 256
                ).alias("record_hash"),
                lit(False).alias("is_deleted")
            )
            
            print("ðŸ”„ DEBUG: Writing to bronze table...")
            # Write to bronze table with more aggressive schema merging
            billing_bronze.write \
                .mode("append") \
                .option("mergeSchema", "true") \
                .option("spark.sql.adaptive.enabled", "true") \
                .option("spark.sql.adaptive.coalescePartitions.enabled", "true") \
                .saveAsTable(f"{self.catalog}.bronze.system_billing_usage")
            
            # Update watermark
            record_count = billing_bronze.count()
            print(f"ðŸ”„ DEBUG: Written {record_count} records to bronze table")
            if record_count > 0:
                latest_timestamp = billing_bronze.select("usage_date").orderBy(col("usage_date").desc()).limit(1).collect()
                if latest_timestamp:
                    # Convert timestamp to string for watermark
                    watermark_value = str(latest_timestamp[0]["usage_date"])
                    self.watermark_manager.update_watermark(
                        "system.billing.usage",
                        "obs.bronze.system_billing_usage",
                        "usage_date",
                        watermark_value,
                        "SUCCESS",
                        None,
                        record_count,
                        0
                    )
                    logger.info(f"Updated watermark for billing usage: {watermark_value}")
                    print(f"âœ… DEBUG: Updated watermark for billing usage: {watermark_value}")
            
            logger.info(f"Billing usage ingested successfully - {record_count} records")
            print(f"âœ… DEBUG: Billing usage ingested successfully - {record_count} records")
            return True
            
        except Exception as e:
            logger.error(f"Error ingesting billing usage: {str(e)}")
            print(f"âŒ DEBUG: Error ingesting billing usage: {str(e)}")
            return False
    
    def _ingest_billing_list_prices(self) -> bool:
        """Ingest billing list prices from system table to bronze."""
        try:
            logger.info("Ingesting billing list prices...")
            print("ðŸ”„ DEBUG: Starting billing list prices ingestion...")
            
            # Get watermark for delta processing
            watermark = self.watermark_manager.get_watermark(
                "system.billing.list_prices",
                "obs.bronze.system_billing_list_prices", 
                "price_start_time"
            )
            
            if watermark is None:
                watermark = "1900-01-01"
                logger.info(f"No watermark found, processing from {watermark}")
                print(f"ðŸ”„ DEBUG: No watermark found, processing from {watermark}")
            else:
                logger.info(f"Processing delta from watermark: {watermark}")
                print(f"ðŸ”„ DEBUG: Processing delta from watermark: {watermark}")
            
                # Read from system table with watermark filter
                try:
                    prices_source = self.spark.table("system.billing.list_prices") \
                        .filter(col("price_start_time") > watermark)
                print(f"ðŸ”„ DEBUG: Read from system.billing.list_prices, checking record count...")
                source_count = prices_source.count()
                print(f"ðŸ”„ DEBUG: Source table has {source_count} records after watermark filter")
            except Exception as e:
                logger.warning(f"System table system.billing.list_prices not accessible: {str(e)}")
                print(f"âŒ DEBUG: System table system.billing.list_prices not accessible: {str(e)}")
                logger.info("Skipping billing list prices ingestion - system table not available")
                return True
            
            if source_count == 0:
                print("â„¹ï¸ DEBUG: No new records to process after watermark filter")
                logger.info("No new billing list prices records to process")
                return True
            
            # Transform to bronze format
            print("ðŸ”„ DEBUG: Transforming data to bronze format...")
            prices_bronze = prices_source.select(
                struct(
                    col("sku_name").alias("sku_name"),
                    col("cloud").alias("cloud"),
                    col("price_start_time").alias("effective_date"),
                    col("unit_price").alias("unit_price"),
                    col("currency").alias("currency"),
                    col("unit").alias("unit")
                ).alias("raw_data"),
                col("cloud"),
                col("price_start_time"),
                current_timestamp().alias("ingestion_timestamp"),
                lit("system.billing.list_prices").alias("source_file"),
                sha2(
                    concat_ws("|",
                        col("sku_name"),
                        col("cloud"),
                        col("price_start_time").cast("string"),
                        col("unit_price").cast("string"),
                        col("currency"),
                        col("unit")
                    ), 256
                ).alias("record_hash"),
                lit(False).alias("is_deleted")
            )
            
            print("ðŸ”„ DEBUG: Writing to bronze table...")
            # Write to bronze table
            prices_bronze.write.mode("append").option("mergeSchema", "true").saveAsTable(f"{self.catalog}.bronze.system_billing_list_prices")
            
            # Update watermark
            record_count = prices_bronze.count()
            print(f"ðŸ”„ DEBUG: Written {record_count} records to bronze table")
            if record_count > 0:
                latest_timestamp = prices_bronze.select("price_start_time").orderBy(col("price_start_time").desc()).limit(1).collect()
                if latest_timestamp:
                    # Convert timestamp to string for watermark
                    watermark_value = str(latest_timestamp[0]["price_start_time"])
                    self.watermark_manager.update_watermark(
                        "system.billing.list_prices",
                        "obs.bronze.system_billing_list_prices",
                        "price_start_time",
                        watermark_value,
                        "SUCCESS",
                        None,
                        record_count,
                        0
                    )
                    logger.info(f"Updated watermark for billing list prices: {watermark_value}")
                    print(f"âœ… DEBUG: Updated watermark for billing list prices: {watermark_value}")
            
            logger.info(f"Billing list prices ingested successfully - {record_count} records")
            print(f"âœ… DEBUG: Billing list prices ingested successfully - {record_count} records")
            return True
            
        except Exception as e:
            logger.error(f"Error ingesting billing list prices: {str(e)}")
            print(f"âŒ DEBUG: Error ingesting billing list prices: {str(e)}")
            return False
    
    def _ingest_query_history(self) -> bool:
        """Ingest query history from system table to bronze."""
        try:
            logger.info("Ingesting query history...")
            print("ðŸ”„ DEBUG: Starting query history ingestion...")
            
            # Get watermark for delta processing
            watermark = self.watermark_manager.get_watermark(
                "system.query.history",
                "obs.bronze.system_query_history", 
                "start_time"
            )
            
            if watermark is None:
                watermark = "1900-01-01"
                logger.info(f"No watermark found, processing from {watermark}")
                print(f"ðŸ”„ DEBUG: No watermark found, processing from {watermark}")
            else:
                logger.info(f"Processing delta from watermark: {watermark}")
                print(f"ðŸ”„ DEBUG: Processing delta from watermark: {watermark}")
            
            # Read from system table with watermark filter
            try:
                query_source = self.spark.table("system.query.history") \
                    .filter(col("start_time") > watermark)
                print(f"ðŸ”„ DEBUG: Read from system.query.history, checking record count...")
                source_count = query_source.count()
                print(f"ðŸ”„ DEBUG: Source table has {source_count} records after watermark filter")
            except Exception as e:
                logger.warning(f"System table system.query.history not accessible: {str(e)}")
                print(f"âŒ DEBUG: System table system.query.history not accessible: {str(e)}")
                logger.info("Skipping query history ingestion - system table not available")
                return True
            
            if source_count == 0:
                print("â„¹ï¸ DEBUG: No new records to process after watermark filter")
                logger.info("No new query history records to process")
                return True
            
            # Transform to bronze format
            print("ðŸ”„ DEBUG: Transforming data to bronze format...")
            query_bronze = query_source.select(
                struct(
                    col("statement_id").alias("statement_id"),
                    col("workspace_id").alias("workspace_id"),
                    col("session_id").alias("session_id"),
                    col("execution_status").alias("execution_status"),
                    col("start_time").alias("start_time"),
                    col("end_time").alias("end_time"),
                    col("duration_ms").alias("duration_ms"),
                    col("query_type").alias("query_type"),
                    col("warehouse_id").alias("warehouse_id"),
                    col("cluster_id").alias("cluster_id")
                ).alias("raw_data"),
                col("workspace_id"),
                col("start_time"),
                current_timestamp().alias("ingestion_timestamp"),
                lit("system.query.history").alias("source_file"),
                sha2(
                    concat_ws("|",
                        col("statement_id"),
                        col("workspace_id"),
                        col("session_id"),
                        col("execution_status"),
                        col("start_time").cast("string"),
                        col("end_time").cast("string"),
                        col("duration_ms").cast("string"),
                        col("query_type"),
                        col("warehouse_id"),
                        col("cluster_id")
                    ), 256
                ).alias("record_hash"),
                lit(False).alias("is_deleted")
            )
            
            print("ðŸ”„ DEBUG: Writing to bronze table...")
            # Write to bronze table
            query_bronze.write.mode("append").option("mergeSchema", "true").saveAsTable(f"{self.catalog}.bronze.system_query_history")
            
            # Update watermark
            record_count = query_bronze.count()
            print(f"ðŸ”„ DEBUG: Written {record_count} records to bronze table")
            if record_count > 0:
                latest_timestamp = query_bronze.select("start_time").orderBy(col("start_time").desc()).limit(1).collect()
                if latest_timestamp:
                    # Convert timestamp to string for watermark
                    watermark_value = str(latest_timestamp[0]["start_time"])
                    self.watermark_manager.update_watermark(
                        "system.query.history",
                        "obs.bronze.system_query_history",
                        "start_time",
                        watermark_value,
                        "SUCCESS",
                        None,
                        record_count,
                        0
                    )
                    logger.info(f"Updated watermark for query history: {watermark_value}")
                    print(f"âœ… DEBUG: Updated watermark for query history: {watermark_value}")
            
            logger.info(f"Query history ingested successfully - {record_count} records")
            print(f"âœ… DEBUG: Query history ingested successfully - {record_count} records")
            return True
            
        except Exception as e:
            logger.error(f"Error ingesting query history: {str(e)}")
            print(f"âŒ DEBUG: Error ingesting query history: {str(e)}")
            return False
    
    def _ingest_audit_log(self) -> bool:
        """Ingest audit log from system table to bronze."""
        try:
            logger.info("Ingesting audit log...")
            print("ðŸ”„ DEBUG: Starting audit log ingestion...")
            
            # Get watermark for delta processing
            watermark = self.watermark_manager.get_watermark(
                "system.access.audit",
                "obs.bronze.system_access_audit", 
                "event_time"
            )
            
            if watermark is None:
                watermark = "1900-01-01"
                logger.info(f"No watermark found, processing from {watermark}")
                print(f"ðŸ”„ DEBUG: No watermark found, processing from {watermark}")
            else:
                logger.info(f"Processing delta from watermark: {watermark}")
                print(f"ðŸ”„ DEBUG: Processing delta from watermark: {watermark}")
            
            # Read from system table with watermark filter
            try:
                audit_source = self.spark.table("system.access.audit") \
                    .filter(col("event_time") > watermark)
                print(f"ðŸ”„ DEBUG: Read from system.access.audit, checking record count...")
                source_count = audit_source.count()
                print(f"ðŸ”„ DEBUG: Source table has {source_count} records after watermark filter")
            except Exception as e:
                logger.warning(f"System table system.access.audit not accessible: {str(e)}")
                print(f"âŒ DEBUG: System table system.access.audit not accessible: {str(e)}")
                logger.info("Skipping audit log ingestion - system table not available")
                return True
            
            if source_count == 0:
                print("â„¹ï¸ DEBUG: No new records to process after watermark filter")
                logger.info("No new audit log records to process")
                return True
            
            # Transform to bronze format
            print("ðŸ”„ DEBUG: Transforming data to bronze format...")
            audit_bronze = audit_source.select(
                struct(
                    col("event_time").alias("timestamp"),
                    col("user_identity").cast("string").alias("user_identity"),
                    col("action_name").alias("action"),
                    col("request_id").alias("resource"),
                    col("response").cast("string").alias("result"),
                    col("workspace_id").alias("workspace_id")
                ).alias("raw_data"),
                col("workspace_id"),
                col("event_time"),
                current_timestamp().alias("ingestion_timestamp"),
                lit("system.access.audit").alias("source_file"),
                sha2(
                    concat_ws("|",
                        col("event_time").cast("string"),
                        col("user_identity").cast("string"),  # Convert STRUCT to string
                        col("action_name"),
                        col("request_id"),
                        col("response").cast("string"),  # Convert STRUCT to string
                        col("workspace_id")
                    ), 256
                ).alias("record_hash"),
                lit(False).alias("is_deleted")
            )
            
            print("ðŸ”„ DEBUG: Writing to bronze table...")
            # Write to bronze table
            audit_bronze.write.mode("append").option("mergeSchema", "true").saveAsTable(f"{self.catalog}.bronze.system_access_audit")
            
            # Update watermark
            record_count = audit_bronze.count()
            print(f"ðŸ”„ DEBUG: Written {record_count} records to bronze table")
            if record_count > 0:
                latest_timestamp = audit_bronze.select("event_time").orderBy(col("event_time").desc()).limit(1).collect()
                if latest_timestamp:
                    # Convert timestamp to string for watermark
                    watermark_value = str(latest_timestamp[0]["event_time"])
                    self.watermark_manager.update_watermark(
                        "system.access.audit",
                        "obs.bronze.system_access_audit",
                        "event_time",
                        watermark_value,
                        "SUCCESS",
                        None,
                        record_count,
                        0
                    )
                    logger.info(f"Updated watermark for audit log: {watermark_value}")
                    print(f"âœ… DEBUG: Updated watermark for audit log: {watermark_value}")
            
            logger.info(f"Audit log ingested successfully - {record_count} records")
            print(f"âœ… DEBUG: Audit log ingested successfully - {record_count} records")
            return True
            
        except Exception as e:
            logger.error(f"Error ingesting audit log: {str(e)}")
            print(f"âŒ DEBUG: Error ingesting audit log: {str(e)}")
            return False
    
    def _ingest_storage_ops(self) -> bool:
        """Ingest storage operations from system table to bronze."""
        try:
            logger.info("Ingesting storage operations...")
            print("ðŸ”„ DEBUG: Starting storage operations ingestion...")
            
            # Get watermark for delta processing
            watermark = self.watermark_manager.get_watermark(
                "system.storage.ops",
                "obs.bronze.system_storage_ops", 
                "timestamp"
            )
            
            if watermark is None:
                watermark = "1900-01-01"
                logger.info(f"No watermark found, processing from {watermark}")
                print(f"ðŸ”„ DEBUG: No watermark found, processing from {watermark}")
            else:
                logger.info(f"Processing delta from watermark: {watermark}")
                print(f"ðŸ”„ DEBUG: Processing delta from watermark: {watermark}")
            
            # Read from system table with watermark filter
            try:
                storage_source = self.spark.table("system.storage.ops") \
                    .filter(col("timestamp") > watermark)
                print(f"ðŸ”„ DEBUG: Read from system.storage.ops, checking record count...")
                source_count = storage_source.count()
                print(f"ðŸ”„ DEBUG: Source table has {source_count} records after watermark filter")
            except Exception as e:
                if "TABLE_OR_VIEW_NOT_FOUND" in str(e):
                    logger.info("System table system.storage.ops does not exist in this workspace")
                    print(f"â„¹ï¸ DEBUG: System table system.storage.ops does not exist in this workspace")
                else:
                    logger.warning(f"System table system.storage.ops not accessible: {str(e)}")
                    print(f"âŒ DEBUG: System table system.storage.ops not accessible: {str(e)}")
                logger.info("Skipping storage operations ingestion - system table not available")
                return True
            
            if source_count == 0:
                print("â„¹ï¸ DEBUG: No new records to process after watermark filter")
                logger.info("No new storage operations records to process")
                return True
            
            # Transform to bronze format
            print("ðŸ”„ DEBUG: Transforming data to bronze format...")
            storage_bronze = storage_source.select(
                struct(
                    col("timestamp").alias("timestamp"),
                    col("workspace_id").alias("workspace_id"),
                    col("operation").alias("operation"),
                    col("path").alias("path"),
                    col("size_bytes").alias("size_bytes"),
                    col("duration_ms").alias("duration_ms"),
                    col("user_identity").alias("user_identity")
                ).alias("raw_data"),
                col("workspace_id"),
                col("timestamp"),
                current_timestamp().alias("ingestion_timestamp"),
                lit("system.storage.ops").alias("source_file"),
                sha2(
                    concat_ws("|",
                        col("timestamp").cast("string"),
                        col("workspace_id"),
                        col("operation"),
                        col("path"),
                        col("size_bytes").cast("string"),
                        col("duration_ms").cast("string"),
                        col("user_identity")
                    ), 256
                ).alias("record_hash"),
                lit(False).alias("is_deleted")
            )
            
            print("ðŸ”„ DEBUG: Writing to bronze table...")
            # Write to bronze table
            storage_bronze.write.mode("append").option("mergeSchema", "true").saveAsTable(f"{self.catalog}.bronze.system_storage_ops")
            
            # Update watermark
            record_count = storage_bronze.count()
            print(f"ðŸ”„ DEBUG: Written {record_count} records to bronze table")
            if record_count > 0:
                latest_timestamp = storage_bronze.select("timestamp").orderBy(col("timestamp").desc()).limit(1).collect()
                if latest_timestamp:
                    # Convert timestamp to string for watermark
                    watermark_value = str(latest_timestamp[0]["timestamp"])
                    self.watermark_manager.update_watermark(
                        "system.storage.ops",
                        "obs.bronze.system_storage_ops",
                        "timestamp",
                        watermark_value,
                        "SUCCESS",
                        None,
                        record_count,
                        0
                    )
                    logger.info(f"Updated watermark for storage operations: {watermark_value}")
                    print(f"âœ… DEBUG: Updated watermark for storage operations: {watermark_value}")
            
            logger.info(f"Storage operations ingested successfully - {record_count} records")
            print(f"âœ… DEBUG: Storage operations ingested successfully - {record_count} records")
            return True
            
        except Exception as e:
            logger.error(f"Error ingesting storage operations: {str(e)}")
            print(f"âŒ DEBUG: Error ingesting storage operations: {str(e)}")
            return False
